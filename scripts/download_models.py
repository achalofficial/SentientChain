import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import login, hf_hub_download

# ‚úÖ Securely get the Hugging Face token from an environment variable
HF_TOKEN = os.getenv("HF_TOKEN")

if not HF_TOKEN:
    raise ValueError("‚ùå Hugging Face token not found. Please set HF_TOKEN in your environment.")

# ‚úÖ Log in to Hugging Face
login(token=HF_TOKEN)

# ‚úÖ Define models to download
models = [
    {"repo_id": "NousResearch/Nous-Hermes-2-Mistral-7B", "filename": "pytorch_model.bin"},
]

# ‚úÖ Create models directory if it doesn't exist
download_dir = "models"
os.makedirs(download_dir, exist_ok=True)

# ‚úÖ Download each model
for model in models:
    try:
        print(f"üîÑ Downloading {model['repo_id']} ...")

        # Download model and tokenizer
        model_path = AutoModelForCausalLM.from_pretrained(model["repo_id"], cache_dir=download_dir)
        tokenizer_path = AutoTokenizer.from_pretrained(model["repo_id"], cache_dir=download_dir)

        print(f"‚úÖ {model['repo_id']} successfully downloaded!")
    except Exception as e:
        print(f"‚ùå ERROR downloading {model['repo_id']}: {e}")
